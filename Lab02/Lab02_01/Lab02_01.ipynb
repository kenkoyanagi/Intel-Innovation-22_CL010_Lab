{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Optimizing Inference for Intel Hybrid Platforms\n",
    "\n",
    "### Prerequisites: \n",
    "- Python3\n",
    "- onnx\n",
    "- onnxruntime\n",
    "- numpy\n",
    "- cv2 \n",
    "\n",
    "### Lab Objectives: \n",
    "- Improve performance with intra/inter thread count configurations\n",
    "- Reduce CPU utilization by setting spin loops to false\n",
    "- Improving performance with INT8 optimizations\n",
    "- Further improving performance of the model with ONNX Runtime optimization levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a baseline of our performance with ResNet50\n",
    "\n",
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ResNet50 model\n",
    "\n",
    "Ensure the model created from Lab 01 is located at the model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../models/resnet50.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Create the runtime session and use *CPUExecutionProvider* to run on CPU with MLAS (Microsoft Linear Algebra Subroutine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available providers\")\n",
    "print(rt.get_available_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(model_path, providers=['CPUExecutionProvider'])     #Use CPU execution provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a sample image to classify the object.\n",
    "\n",
    ">Find the sample image under resources/ and read the image in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "img = cv2.imread(\"../../resources/lab03_image.jpg\")\n",
    "cv2.imshow(\"lab03_img\", img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyWindow('lab03_img')\n",
    "\n",
    "# Preprocess the image for ResNet50. \n",
    "def preprocess(img):\n",
    "    img = img / 255.\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    y0 = (h - 224) // 2\n",
    "    x0 = (w - 224) // 2\n",
    "    img = img[y0 : y0+224, x0 : x0+224, :]\n",
    "    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "    img = np.transpose(img, axes=[2, 0, 1])\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "img = preprocess(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference with the sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = sess.get_inputs()[0].name \n",
    "output_name = sess.get_outputs()[0].name \n",
    "\n",
    "prediction = sess.run(None, {input_name: img})[0]\n",
    "prediction = np.squeeze(prediction)\n",
    "top = np.argsort(prediction)[::-1]\n",
    "\n",
    "# Read in human-readable class labels \n",
    "with open(\"../../resources/labels.txt\", 'r') as f:\n",
    "    labels = [l.rstrip() for l in f]\n",
    "\n",
    "index = top[0]\n",
    "print(index)\n",
    "print(\"Predicted class:{0}  Probability: {1}\".format(labels[index], prediction[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an idea of our model's baseline performance by running the prediction loop 100x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    prediction = sess.run(None, {input_name: img})[0]\n",
    "end = time.time()\n",
    "print(\"Total execution time of 100 inference sessions: {:.3f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our baseline performance with default options with ONNXRuntime, we will explore some of the optimizations that improve performance on hybrid systems. \n",
    "\n",
    "Let's improve performance with intra/inter thread count controls.\n",
    "\n",
    ">For this lab, let's experiment with changing the value for intra op num threads to see how this impacts the performance. \n",
    "\n",
    "For additional help, please refer to: Thread Tuning - Low Precision Tuning section for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session options to change different knobs for CPU (MLAS)\n",
    "'''\n",
    "    TODO: Create SessionOptions() object\n",
    "    TODO: Change the number of threads\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    TODO: Select the execution mode type, use ORT_SEQUENTIAL\n",
    "    TODO: Use GraphOptimizationLevel.ORT_ENABLE_BASIC\n",
    "    TODO: Create the session\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(100):\n",
    "    prediction = sess.run(None, {input_name: img})[0]\n",
    "end = time.time()\n",
    "print(\"Total execution time of 100 inference sessions: {:.3f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take advantage of ONNX Runtime's graph optimizations by setting it to all. \\\n",
    ">Change the Graph Optimization level to Enable All (ORT_ENABLE_ALL) \\\n",
    ">Hint: Try typing rt. and see what auto-completion brings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_options.graph_optimization_level = '''TODO: Change to ORT_ENABLE_ALL configuration'''\n",
    "\n",
    "sess = rt.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"], sess_options=sess_options)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    prediction = sess.run(None, {input_name: img})[0]\n",
    "end = time.time()\n",
    "print(\"Total execution time of 100 inference sessions: {:.3f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll put everything together alongside the Int8 model we've created in Lab01_02\n",
    "\n",
    ">Apply all of the optimizations learned for the inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../models/resnet50_int8.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "del sess_options\n",
    "\n",
    "#Use all of the optimization techniques learned here\n",
    "'''\n",
    "    TODO: Apply all of the optimization techniques learned by \n",
    "    1. creating a SessionOptions() object\n",
    "    2. Enabling all graph optimizations\n",
    "    3. Using a performant number of threads\n",
    "    4. Using CPU as the provider with the Int8 Quantized model\n",
    "'''\n",
    "\n",
    "input_name = sess.get_inputs()[0].name \n",
    "output_name = sess.get_outputs()[0].name \n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    prediction = sess.run(None, {input_name: img})[0]\n",
    "end = time.time()\n",
    "print(\"Total execution time of 100 inference sessions: {:.3f} seconds\".format(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "07abc5c12c121f92e047b7f0a8fc168c559bedd916698625afeb4a7126365e32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
