{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783b6cef",
   "metadata": {},
   "source": [
    "## Lab01_02: Using IntelÂ® Neural Compressor to quantize a model\n",
    "\n",
    "### Prerequisites: \n",
    "- onnx\n",
    "- onnxruntime \n",
    "- onnxruntime-extensions\n",
    "- neural_compressor \n",
    "- numpy \n",
    "\n",
    "### Objective:\n",
    "- Learn the basics of the quantization pipeline (API, dataset, configuration file, quantization type)\n",
    "- Evaluate the accuracy difference between dynamic quantization and accuracy aware quantization \n",
    "- Quantize ResNet50 exported from Lab01_01 to produce quantized Int8 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5fd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from neural_compressor.experimental import Quantization, common\n",
    "from neural_compressor import options\n",
    "\n",
    "options.onnxrt.graph_optimization.level = 'ENABLE_BASIC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de6eb4",
   "metadata": {},
   "source": [
    "Load the previously generated Float32 ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9588211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"../../models/resnet50.onnx\")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c720c",
   "metadata": {},
   "source": [
    "Here, we'll try two common types of quantization schemes:\n",
    "- Dynamic Quantization, which does not require a dataset, and therefore less code/configuration\n",
    "- Accuracy Aware Quantization, which DOES require a dataset, but typically results in better accuracy\n",
    "\n",
    ">Load each configuration 'yaml' files and run the quantization function. Compare both results. \\\n",
    "    - Dynamic Quantization = **resnet50_v1_5_dynamicq.yaml** \\\n",
    "    - Accuracy Aware Quantization = **resnet50_v1_5_qdq.yaml** \\\n",
    "    - The dataset and label file can be located in the resources/ folder \\\n",
    "    - Make the changes in **resnet50_v1_5_qdq.yaml** to reference the filepath to both the dataset folder and the label file\n",
    "\n",
    "> Dynamic Quantization does not require a sample dataset and label file for calibration, but we are referencing it for evaluating how well the quantization scheme performed. \n",
    "\n",
    ">Static Quantization method will require a sample dataset and label file for calibrating the model. \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2df9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize = Quantization(\"resnet50_v1_5_dynamicq.yaml\") # Configuration .yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e753f",
   "metadata": {},
   "source": [
    ">Quantize the model and save it to the models folder\n",
    "\n",
    ">Use the [quantize() and save()](https://github.com/intel/neural-compressor/tree/master/examples/tensorflow/image_recognition/tensorflow_models/quantization/ptq#code-update) APIs\n",
    "\n",
    "Refer to the slidedeck for additional help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fa5498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 20:02:36 [INFO] Because both eval_dataloader_cfg and user-defined eval_func are None, automatically setting 'tuning.exit_policy.performance_only = True'.\n",
      "2022-09-27 20:02:36 [INFO] Generate a fake evaluation function.\n",
      "2022-09-27 20:02:37 [INFO] Get FP32 model baseline.\n",
      "2022-09-27 20:02:37 [INFO] Save tuning history to C:\\Users\\Intel\\Desktop\\NEW\\Intel-Innovation-22_CL010_Lab\\Lab01\\Lab01_02\\nc_workspace\\2022-09-27_20-02-30\\./history.snapshot.\n",
      "2022-09-27 20:02:37 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2022-09-27 20:02:40 [INFO] |********Mixed Precision Statistics********|\n",
      "2022-09-27 20:02:40 [INFO] +-------------------------+--------+-------+\n",
      "2022-09-27 20:02:40 [INFO] |         Op Type         | Total  |  INT8 |\n",
      "2022-09-27 20:02:40 [INFO] +-------------------------+--------+-------+\n",
      "2022-09-27 20:02:40 [INFO] |          MatMul         |   1    |   1   |\n",
      "2022-09-27 20:02:40 [INFO] |           Conv          |   53   |   53  |\n",
      "2022-09-27 20:02:40 [INFO] |  DynamicQuantizeLinear  |   50   |   50  |\n",
      "2022-09-27 20:02:40 [INFO] +-------------------------+--------+-------+\n",
      "2022-09-27 20:02:40 [INFO] Pass quantize model elapsed time: 2873.89 ms\n",
      "2022-09-27 20:02:40 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2022-09-27 20:02:40 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2022-09-27 20:02:40 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-27 20:02:40 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2022-09-27 20:02:40 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-27 20:02:40 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
      "2022-09-27 20:02:40 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
      "2022-09-27 20:02:40 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-27 20:02:40 [INFO] Save tuning history to C:\\Users\\Intel\\Desktop\\NEW\\Intel-Innovation-22_CL010_Lab\\Lab01\\Lab01_02\\nc_workspace\\2022-09-27_20-02-30\\./history.snapshot.\n",
      "2022-09-27 20:02:40 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2022-09-27 20:02:40 [INFO] Save deploy yaml to C:\\Users\\Intel\\Desktop\\NEW\\Intel-Innovation-22_CL010_Lab\\Lab01\\Lab01_02\\nc_workspace\\2022-09-27_20-02-30\\deploy.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 Quantized model saved!\n"
     ]
    }
   ],
   "source": [
    "quantize.model = common.Model(model)\n",
    "q_model = quantize()\n",
    "# Save the quantized model under the models/ folder\n",
    "q_model.save(\"../../models/resnet50_int8.onnx\")\n",
    "print(\"Int8 Quantized model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a5723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
